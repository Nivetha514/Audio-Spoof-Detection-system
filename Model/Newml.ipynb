{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Define paths to your CSV datasets\n",
    "dataset_path = \"DATASET-balanced.csv\"\n",
    "\n",
    "# Read data from CSV files\n",
    "df = pd.read_csv(dataset_path)\n",
    "# Define desired size for datasets\n",
    "target_size = 2000 \n",
    "\n",
    "# Stratified sampling to maintain class balance\n",
    "smaller_df = df.sample(target_size, random_state=42) \n",
    "\n",
    "# Combine features and labels from both datasets\n",
    "features = smaller_df.drop(\"Classname\", axis=1)\n",
    "labels = smaller_df[\"Classname\"]\n",
    "\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2)\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_train_encoded = le.fit_transform(y_train)\n",
    "y_test_encoded = le.fit_transform(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.98\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98       196\n",
      "           1       0.98      0.99      0.98       204\n",
      "\n",
      "    accuracy                           0.98       400\n",
      "   macro avg       0.98      0.98      0.98       400\n",
      "weighted avg       0.98      0.98      0.98       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "clf_1 = RandomForestClassifier( \n",
    "    n_estimators=100,\n",
    "    criterion='gini',\n",
    "    max_depth=None,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    max_features='sqrt',\n",
    "    bootstrap=False,\n",
    "    oob_score=False,\n",
    "    class_weight=None,\n",
    "    ccp_alpha=0.0)\n",
    "\n",
    "\n",
    "# Train the classifier using the training data\n",
    "clf_1.fit(X_train, y_train_encoded)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = clf_1.predict(X_test)\n",
    "\n",
    "# Evaluate the classifier's performance\n",
    "accuracy = accuracy_score(y_test_encoded, y_pred)\n",
    "report = classification_report(y_test_encoded, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.98\n",
      "EER: 0.00\n",
      "AUC: 0.98\n",
      "TDCF for model: 52\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, f1_score, roc_auc_score, roc_curve, auc\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(y_test_encoded, y_pred)\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "\n",
    "# Calculate EER (Equal Error Rate)\n",
    "fpr, tpr, thresholds = roc_curve(y_test_encoded, clf_1.predict_proba(X_test)[:, 1])\n",
    "eer = fpr[np.argmin(np.absolute((fpr - tpr)))]\n",
    "print(f\"EER: {eer:.2f}\")\n",
    "\n",
    "# Optionally, calculate AUC (Area Under the ROC Curve)\n",
    "auc = roc_auc_score(y_test_encoded, y_pred)\n",
    "print(f\"AUC: {auc:.2f}\")\n",
    "\n",
    "tn1, fp1, fn1, tp1 = confusion_matrix(y_test_encoded, y_pred, labels=[0, 1]).ravel()\n",
    "TDCF1 = 10*fp1 + fn1\n",
    "print(f'TDCF for model: {TDCF1}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.96\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.96      0.96       196\n",
      "           1       0.97      0.96      0.96       204\n",
      "\n",
      "    accuracy                           0.96       400\n",
      "   macro avg       0.96      0.96      0.96       400\n",
      "weighted avg       0.96      0.96      0.96       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_2 = GradientBoostingClassifier()\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y_train_encoded = le.fit_transform(y_train)\n",
    "\n",
    "y_test_encoded = le.fit_transform(y_test)\n",
    "# Train the classifier using the training data\n",
    "clf_2.fit(X_train, y_train_encoded)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = clf_2.predict(X_test)\n",
    "\n",
    "# Evaluate the classifier's performance\n",
    "accuracy = accuracy_score(y_test_encoded, y_pred)\n",
    "report = classification_report(y_test_encoded, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.96\n",
      "EER: 0.00\n",
      "AUC: 0.96\n",
      "TDCF for model: 79\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, f1_score, roc_auc_score, roc_curve, auc\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(y_test_encoded, y_pred)\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "\n",
    "# Calculate EER (Equal Error Rate)\n",
    "fpr, tpr, thresholds = roc_curve(y_test_encoded, clf_2.predict_proba(X_test)[:, 1])\n",
    "eer = fpr[np.argmin(np.absolute((fpr - tpr)))]\n",
    "print(f\"EER: {eer:.2f}\")\n",
    "\n",
    "# Optionally, calculate AUC (Area Under the ROC Curve)\n",
    "auc = roc_auc_score(y_test_encoded, y_pred)\n",
    "print(f\"AUC: {auc:.2f}\")\n",
    "\n",
    "tn1, fp1, fn1, tp1 = confusion_matrix(y_test_encoded, y_pred, labels=[0, 1]).ravel()\n",
    "TDCF1 = 10*fp1 + fn1\n",
    "print(f'TDCF for model: {TDCF1}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.99\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99       196\n",
      "           1       0.99      0.99      0.99       204\n",
      "\n",
      "    accuracy                           0.99       400\n",
      "   macro avg       0.99      0.99      0.99       400\n",
      "weighted avg       0.99      0.99      0.99       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_3 =CatBoostClassifier(verbose=0)\n",
    "\n",
    "# Train the classifier using the training data\n",
    "clf_3.fit(X_train, y_train_encoded)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = clf_3.predict(X_test)\n",
    "\n",
    "# Evaluate the classifier's performance\n",
    "accuracy = accuracy_score(y_test_encoded, y_pred)\n",
    "report = classification_report(y_test_encoded, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.99\n",
      "EER: 0.00\n",
      "AUC: 0.99\n",
      "TDCF for model: 32\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, f1_score, roc_auc_score, roc_curve, auc\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(y_test_encoded, y_pred)\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "\n",
    "# Calculate EER (Equal Error Rate)\n",
    "fpr, tpr, thresholds = roc_curve(y_test_encoded, clf_3.predict_proba(X_test)[:, 1])\n",
    "eer = fpr[np.argmin(np.absolute((fpr - tpr)))]\n",
    "print(f\"EER: {eer:.2f}\")\n",
    "\n",
    "# Optionally, calculate AUC (Area Under the ROC Curve)\n",
    "auc = roc_auc_score(y_test_encoded, y_pred)\n",
    "print(f\"AUC: {auc:.2f}\")\n",
    "\n",
    "tn1, fp1, fn1, tp1 = confusion_matrix(y_test_encoded, y_pred, labels=[0, 1]).ravel()\n",
    "TDCF1 = 10*fp1 + fn1\n",
    "print(f'TDCF for model: {TDCF1}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "# Convert the dataset into DMatrix format (required by XGBoost)\n",
    "dtrain = xgboost.DMatrix(X_train, label=y_train_encoded)\n",
    "dtest = xgboost.DMatrix(X_test, label=y_test_encoded)\n",
    "params = {\n",
    "    \"objective\": \"multi:softmax\",\n",
    "    \"num_class\": 3,\n",
    "    \"max_depth\": 3,\n",
    "    \"eta\": 0.3\n",
    "}\n",
    "# Train the XGBoost model\n",
    "num_round = 1000\n",
    "clf_4 = xgboost.train(params,dtrain, num_round)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = clf_4.predict(dtest)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test_encoded, y_pred)\n",
    "print(f\"XGBoost Accuracy: {accuracy:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.98\n",
      "EER: 0.00\n",
      "AUC: 0.98\n",
      "TDCF for model: 53\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, f1_score, roc_auc_score, roc_curve, auc\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(y_test_encoded, y_pred)\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "\n",
    "# Calculate EER (Equal Error Rate)\n",
    "fpr, tpr, thresholds = roc_curve(y_test_encoded, clf_4.predict_proba(X_test)[:, 1])\n",
    "eer = fpr[np.argmin(np.absolute((fpr - tpr)))]\n",
    "print(f\"EER: {eer:.2f}\")\n",
    "\n",
    "# Optionally, calculate AUC (Area Under the ROC Curve)\n",
    "auc = roc_auc_score(y_test_encoded, y_pred)\n",
    "print(f\"AUC: {auc:.2f}\")\n",
    "\n",
    "tn1, fp1, fn1, tp1 = confusion_matrix(y_test_encoded, y_pred, labels=[0, 1]).ravel()\n",
    "TDCF1 = 10*fp1 + fn1\n",
    "print(f'TDCF for model: {TDCF1}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.98\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98       196\n",
      "           1       0.98      0.99      0.98       204\n",
      "\n",
      "    accuracy                           0.98       400\n",
      "   macro avg       0.98      0.98      0.98       400\n",
      "weighted avg       0.98      0.98      0.98       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "\n",
    "clf_5 = LGBMClassifier()\n",
    "\n",
    "# Train the classifier using the training data\n",
    "clf_5.fit(X_train, y_train_encoded)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = clf_5.predict(X_test)\n",
    "\n",
    "# Evaluate the classifier's performance\n",
    "accuracy = accuracy_score(y_test_encoded, y_pred)\n",
    "report = classification_report(y_test_encoded, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.98\n",
      "EER: 0.00\n",
      "AUC: 0.98\n",
      "TDCF for model: 52\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, f1_score, roc_auc_score, roc_curve, auc\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(y_test_encoded, y_pred)\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "\n",
    "# Calculate EER (Equal Error Rate)\n",
    "fpr, tpr, thresholds = roc_curve(y_test_encoded, clf_5.predict_proba(X_test)[:, 1])\n",
    "eer = fpr[np.argmin(np.absolute((fpr - tpr)))]\n",
    "print(f\"EER: {eer:.2f}\")\n",
    "\n",
    "# Optionally, calculate AUC (Area Under the ROC Curve)\n",
    "auc = roc_auc_score(y_test_encoded, y_pred)\n",
    "print(f\"AUC: {auc:.2f}\")\n",
    "\n",
    "tn1, fp1, fn1, tp1 = confusion_matrix(y_test_encoded, y_pred, labels=[0, 1]).ravel()\n",
    "TDCF1 = 10*fp1 + fn1\n",
    "print(f'TDCF for model: {TDCF1}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [\n",
    "    (\"RandomForest\", clf_1),\n",
    "    (\"GradientBoost\", clf_2),\n",
    "    (\"CatBoost\", clf_3)#,\n",
    "   # (\"XGBoost\",clf_4),\n",
    "   # (\"Lgbm\",clf_5)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier, StackingClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Create the ensembles\n",
    "voting_clf = VotingClassifier(estimators=estimators, voting=\"soft\")\n",
    "stacking_clf = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())  # Pass an instance of LogisticRegression\n",
    "base_model = RandomForestClassifier()\n",
    "bagging_clf = BaggingClassifier(base_model, n_estimators=10, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting (Soft) Ensemble Accuracy: 0.98\n",
      "Stacking Ensemble Accuracy: 0.99\n",
      "Bagging Ensemble Accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "# Train the ensembles\n",
    "voting_clf.fit(X_train, y_train_encoded)\n",
    "stacking_clf.fit(X_train, y_train_encoded)\n",
    "bagging_clf.fit(X_train, y_train_encoded)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_voting = voting_clf.predict(X_test)\n",
    "y_pred_stacking = stacking_clf.predict(X_test)\n",
    "y_pred_bagging = bagging_clf.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy of the ensemble\n",
    "accuracy_voting = accuracy_score(y_test_encoded, y_pred_voting)\n",
    "print(f\"Voting (Soft) Ensemble Accuracy: {accuracy_voting:.2f}\")\n",
    "accuracy_stacking = accuracy_score(y_test_encoded, y_pred_stacking)\n",
    "print(f\"Stacking Ensemble Accuracy: {accuracy_stacking:.2f}\")\n",
    "accuracy_bagging = accuracy_score(y_test_encoded, y_pred_bagging)\n",
    "print(f\"Bagging Ensemble Accuracy: {accuracy_bagging:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voting (Soft) Ensemble Accuracy: 0.97\n",
    "Stacking Ensemble Accuracy: 0.97\n",
    "Bagging Ensemble Accuracy: 0.96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.99\n",
      "EER: 0.00\n",
      "AUC: 0.98\n",
      "TDCF for model: 42\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, f1_score, roc_auc_score, roc_curve, auc\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(y_test_encoded, y_pred_voting)\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "\n",
    "# Calculate EER (Equal Error Rate)\n",
    "fpr, tpr, thresholds = roc_curve(y_test_encoded, voting_clf.predict_proba(X_test)[:, 1])\n",
    "eer = fpr[np.argmin(np.absolute((fpr - tpr)))]\n",
    "print(f\"EER: {eer:.2f}\")\n",
    "\n",
    "# Optionally, calculate AUC (Area Under the ROC Curve)\n",
    "auc = roc_auc_score(y_test_encoded, y_pred_voting)\n",
    "print(f\"AUC: {auc:.2f}\")\n",
    "\n",
    "tn1, fp1, fn1, tp1 = confusion_matrix(y_test_encoded, y_pred_voting, labels=[0, 1]).ravel()\n",
    "TDCF1 = 10*fp1 + fn1\n",
    "print(f'TDCF for model: {TDCF1}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.99\n",
      "EER: 0.00\n",
      "AUC: 0.99\n",
      "TDCF for model: 31\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, f1_score, roc_auc_score, roc_curve, auc\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(y_test_encoded, y_pred_stacking)\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "\n",
    "# Calculate EER (Equal Error Rate)\n",
    "fpr, tpr, thresholds = roc_curve(y_test_encoded, stacking_clf.predict_proba(X_test)[:, 1])\n",
    "eer = fpr[np.argmin(np.absolute((fpr - tpr)))]\n",
    "print(f\"EER: {eer:.2f}\")\n",
    "\n",
    "# Optionally, calculate AUC (Area Under the ROC Curve)\n",
    "auc = roc_auc_score(y_test_encoded, y_pred_stacking)\n",
    "print(f\"AUC: {auc:.2f}\")\n",
    "\n",
    "tn1, fp1, fn1, tp1 = confusion_matrix(y_test_encoded, y_pred_stacking, labels=[0, 1]).ravel()\n",
    "TDCF1 = 10*fp1 + fn1\n",
    "print(f'TDCF for model: {TDCF1}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.99\n",
      "EER: 0.00\n",
      "AUC: 0.97\n",
      "TDCF for model: 67\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, f1_score, roc_auc_score, roc_curve, auc\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(y_test_encoded, y_pred_voting)\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "\n",
    "# Calculate EER (Equal Error Rate)\n",
    "fpr, tpr, thresholds = roc_curve(y_test_encoded, bagging_clf.predict_proba(X_test)[:, 1])\n",
    "eer = fpr[np.argmin(np.absolute((fpr - tpr)))]\n",
    "print(f\"EER: {eer:.2f}\")\n",
    "\n",
    "# Optionally, calculate AUC (Area Under the ROC Curve)\n",
    "auc = roc_auc_score(y_test_encoded, y_pred_bagging)\n",
    "print(f\"AUC: {auc:.2f}\")\n",
    "\n",
    "tn1, fp1, fn1, tp1 = confusion_matrix(y_test_encoded, y_pred_bagging, labels=[0, 1]).ravel()\n",
    "TDCF1 = 10*fp1 + fn1\n",
    "print(f'TDCF for model: {TDCF1}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
